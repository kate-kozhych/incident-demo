{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-16T20:43:55.468964400Z",
     "start_time": "2026-02-16T20:43:55.249304800Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import json\n",
    "\n",
    "DATASET_VERSION = 'v2'\n",
    "\n",
    "if DATASET_VERSION == 'baseline':\n",
    "    dataset_path = '../data/synthetic_data_baseline.csv'\n",
    "    intervals_path = '../src/incident_intervals_baseline.json'\n",
    "else:\n",
    "    dataset_path = '../data/synthetic_data_v2_improved.csv'\n",
    "    intervals_path = '../src/incident_intervals_v2_improved.json'\n",
    "\n",
    "df = pd.read_csv(dataset_path)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "with open(intervals_path, 'r') as f:\n",
    "    incident_intervals = json.load(f)\n",
    "\n",
    "test_start = int(len(df) * 0.85)\n",
    "df_test = df.iloc[test_start:].copy()\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTest set: {len(df_test)} timesteps\")\n",
    "print(f\"Test incidents: {(df_test['incident_label'] == 1).sum()} timesteps ({(df_test['incident_label'] == 1).sum()/len(df_test)*100:.2f}%)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: 3000 timesteps\n",
      "Test incidents: 275 timesteps (9.17%)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:10:20.284614100Z",
     "start_time": "2026-02-16T21:10:20.250355700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_start = int(len(df) * 0.85)\n",
    "test_end = len(df)\n",
    "\n",
    "events_test = [\n",
    "    inc for inc in incident_intervals\n",
    "    if int(inc[\"end_idx\"]) > test_start and int(inc[\"start_idx\"]) < test_end\n",
    "]\n",
    "\n",
    "print(\"Test incident EVENTS:\", len(events_test))\n",
    "print(\"First 5 events:\", [(e[\"start_idx\"], e[\"end_idx\"], e.get(\"scenario\",\"\")) for e in events_test[:5]])\n",
    "\n"
   ],
   "id": "901ce7ccc65c1b35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test incident EVENTS: 4\n",
      "First 5 events: [(17416, 17459, 'service_crash'), (17968, 17996, 'traffic_spike'), (18651, 18839, 'traffic_spike'), (19671, 19687, 'backend_failure')]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:45:55.584945100Z",
     "start_time": "2026-02-16T21:45:55.519420300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaselineRule:\n",
    "    \"\"\"\n",
    "    - Alert if ANY metric exceeds its threshold\n",
    "    - Optional: require k consecutive violations (stability filter)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        thresholds_up: dict,\n",
    "        k_consecutive: int = 1,\n",
    "        request_rate_drop: dict | None = None\n",
    "    ):\n",
    "        self.thresholds_up = thresholds_up\n",
    "        self.k_consecutive = k_consecutive\n",
    "        self.request_rate_drop = request_rate_drop\n",
    "\n",
    "    def predict(self, df):\n",
    "        n = len(df)\n",
    "        violations = np.zeros(n, dtype=int)\n",
    "        for metric, thr in self.thresholds_up.items():\n",
    "            if metric in df.columns:\n",
    "                violations += (df[metric].to_numpy() > thr).astype(int)\n",
    "\n",
    "        alerts = (violations > 0)\n",
    "\n",
    "        # DOWN detector for request_rate\n",
    "        if self.request_rate_drop is not None:\n",
    "            col = self.request_rate_drop.get(\"column\", \"request_rate\")\n",
    "            window = int(self.request_rate_drop.get(\"window\", 60))\n",
    "            ratio = float(self.request_rate_drop.get(\"ratio\", 0.4))\n",
    "\n",
    "            if col in df.columns:\n",
    "                rr = df[col].to_numpy(dtype=float)\n",
    "\n",
    "                # rolling median baseline (shifted by 1 to avoid looking at current point)\n",
    "                rr_med = (\n",
    "                    pd.Series(rr)\n",
    "                      .rolling(window=window, min_periods=window)\n",
    "                      .median()\n",
    "                      .shift(1)\n",
    "                      .to_numpy()\n",
    "                )\n",
    "\n",
    "                rr_drop = (rr_med > 0) & (rr < ratio * rr_med)\n",
    "                alerts = alerts | rr_drop  # OR with existing alerts\n",
    "\n",
    "        # k-consecutive confirmation (optional)\n",
    "        alerts = alerts.astype(int)\n",
    "\n",
    "        if self.k_consecutive > 1:\n",
    "            filtered = np.zeros(n, dtype=int)\n",
    "            for i in range(n - self.k_consecutive + 1):\n",
    "                if alerts[i:i + self.k_consecutive].all():\n",
    "                    filtered[i] = 1\n",
    "            alerts = filtered\n",
    "\n",
    "        return np.asarray(alerts, dtype=int)\n",
    "\n",
    "    def __repr__(self):\n",
    "        up = \", \".join([f\"{k}>{v}\" for k, v in self.thresholds_up.items()])\n",
    "        if self.request_rate_drop:\n",
    "            d = self.request_rate_drop\n",
    "            drop_str = f\"{d.get('column','request_rate')}<({d.get('ratio',0.4)}×median_{d.get('window',60)}m)\"\n",
    "            return f\"BaselineRule({up}, +DOWN:{drop_str}, k={self.k_consecutive})\"\n",
    "        return f\"BaselineRule({up}, k={self.k_consecutive})\"\n",
    "\n",
    "\n",
    "thresholds_simple = {\n",
    "    \"cpu_utilization\": 85,\n",
    "    \"memory_usage\": 85,\n",
    "    \"request_latency\": 250,\n",
    "    \"error_rate\": 10,\n",
    "}\n",
    "\n",
    "baseline_simple = BaselineRule(\n",
    "    thresholds_up=thresholds_simple,\n",
    "    k_consecutive=1,\n",
    "    request_rate_drop={\n",
    "        \"column\": \"request_rate\",\n",
    "        \"window\": 60,\n",
    "        \"ratio\": 0.4\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Baseline Rule (Simple Threshold):\")\n",
    "print(f\"  {baseline_simple}\")\n",
    "print(f\"\\nThresholds:\")\n",
    "for metric, thresh in thresholds_simple.items():\n",
    "    print(f\"  {metric}: > {thresh}\")"
   ],
   "id": "d2d02599879980fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Rule (Simple Threshold):\n",
      "  BaselineRule(cpu_utilization>85, memory_usage>85, request_latency>250, error_rate>10, +DOWN:request_rate<(0.4×median_60m), k=1)\n",
      "\n",
      "Thresholds:\n",
      "  cpu_utilization: > 85\n",
      "  memory_usage: > 85\n",
      "  request_latency: > 250\n",
      "  error_rate: > 10\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:46:03.520794500Z",
     "start_time": "2026-02-16T21:46:03.488983700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaselineRuleVoting:\n",
    "    \"\"\"\n",
    "    - Alert if ANY critical metric violates threshold\n",
    "    - OR if votes_required metrics violate thresholds\n",
    "    - Optional k_consecutive confirmation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, thresholds, votes_required=2, k_consecutive=1,\n",
    "                 critical_metrics=(\"error_rate\", \"request_latency\")):\n",
    "        self.thresholds = thresholds\n",
    "        self.votes_required = votes_required\n",
    "        self.k_consecutive = k_consecutive\n",
    "        self.critical_metrics = set(critical_metrics)\n",
    "\n",
    "    def predict(self, df: pd.DataFrame) -> np.ndarray:\n",
    "        n = len(df)\n",
    "        votes = np.zeros(n, dtype=int)\n",
    "        critical = np.zeros(n, dtype=bool)\n",
    "\n",
    "        for metric, thr in self.thresholds.items():\n",
    "            if metric not in df.columns:\n",
    "                continue\n",
    "\n",
    "            v = (df[metric].to_numpy() > thr)\n",
    "\n",
    "            if metric in self.critical_metrics:\n",
    "                critical |= v\n",
    "\n",
    "            votes += v.astype(int)\n",
    "\n",
    "        voting = (votes >= self.votes_required)\n",
    "\n",
    "        # apply k only to voting (NOT to critical)\n",
    "        if self.k_consecutive > 1:\n",
    "            s = pd.Series(voting.astype(int))\n",
    "            voting = (s.rolling(self.k_consecutive).sum() == self.k_consecutive).to_numpy()\n",
    "\n",
    "        alerts = (critical | voting).astype(int)\n",
    "        return alerts\n",
    "\n",
    "    def __repr__(self):\n",
    "        thresh_str = ', '.join([f'{k}>{v}' for k, v in self.thresholds.items()])\n",
    "        crit_str = ', '.join(sorted(self.critical_metrics))\n",
    "        return (f\"BaselineRuleVoting({thresh_str}, votes≥{self.votes_required}, \"\n",
    "                f\"k={self.k_consecutive}, critical=[{crit_str}])\")\n",
    "\n",
    "\n",
    "\n",
    "thresholds_voting = {\n",
    "    'cpu_utilization': 80,       # Slightly lower thresholds\n",
    "    'memory_usage': 80,\n",
    "    'request_latency': 200,\n",
    "    'error_rate': 8,\n",
    "    #'request_rate': 1800,\n",
    "}\n",
    "\n",
    "baseline_voting = BaselineRuleVoting(\n",
    "    thresholds_voting,\n",
    "    votes_required=2,\n",
    "    k_consecutive=2,\n",
    "    critical_metrics=(\"error_rate\", \"request_latency\")\n",
    ")\n",
    "\n",
    "print(f\"Baseline Rule (Voting System):\")\n",
    "print(f\"  {baseline_voting}\")\n",
    "print(f\"\\nLogic: Alert if (critical metric) OR (≥{baseline_voting.votes_required} metrics) \"\n",
    "      f\"for ≥{baseline_voting.k_consecutive} consecutive minutes\")"
   ],
   "id": "fc86d0ffd3e44142",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Rule (Voting System):\n",
      "  BaselineRuleVoting(cpu_utilization>80, memory_usage>80, request_latency>200, error_rate>8, votes≥2, k=2, critical=[error_rate, request_latency])\n",
      "\n",
      "Logic: Alert if (critical metric) OR (≥2 metrics) for ≥2 consecutive minutes\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:53:56.798531900Z",
     "start_time": "2026-02-16T21:53:56.750874800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_alert_events(y_pred) -> int:\n",
    "    y = np.asarray(y_pred, dtype=int)\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    return int(((y[1:] == 1) & (y[:-1] == 0)).sum() + (y[0] == 1))\n",
    "\n",
    "def alert_events_per_day(y_pred: np.ndarray) -> float:\n",
    "    return count_alert_events(y_pred) / len(y_pred) * 1440\n",
    "\n",
    "def event_recall_from_intervals(y_pred, incident_intervals_json, test_start, test_end):\n",
    "    # incident detected if ANY alert=1 during its interval overlap with test\n",
    "    detected = 0\n",
    "    total = 0\n",
    "\n",
    "    for inc in incident_intervals_json:\n",
    "        s = int(inc[\"start_idx\"])\n",
    "        e = int(inc[\"end_idx\"])\n",
    "        if e <= test_start or s >= test_end:\n",
    "            continue\n",
    "\n",
    "        total += 1\n",
    "        local_s = max(0, s - test_start)\n",
    "        local_e = min(test_end - test_start, e - test_start)\n",
    "\n",
    "        if y_pred[local_s:local_e].any():\n",
    "            detected += 1\n",
    "\n",
    "    return detected, total, detected / max(total, 1)\n",
    "\n",
    "def evaluate_baseline_clean(rule, df, incident_intervals_json, test_start, test_end, name):\n",
    "    df_test = df.iloc[test_start:test_end].copy()\n",
    "    y_true = df_test[\"incident_label\"].to_numpy()\n",
    "    y_pred = np.asarray(rule.predict(df_test), dtype=int)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "\n",
    "    minute_alerts_per_day = y_pred.sum() / len(y_pred) * 1440\n",
    "    event_alerts = count_alert_events(y_pred)\n",
    "    event_alerts_day = alert_events_per_day(y_pred)\n",
    "\n",
    "    det, tot, inc_event_recall = event_recall_from_intervals(\n",
    "        y_pred, incident_intervals_json, test_start, test_end\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"EVALUATING: {name}\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Rule: {rule}\")\n",
    "    print(f\"Precision={precision:.3f} Recall(timestep)={recall:.3f} F1={f1:.3f}\")\n",
    "    print(f\"TN={tn} FP={fp} FN={fn} TP={tp}\")\n",
    "\n",
    "    print(\"\\nAlert rate:\")\n",
    "    print(f\"  Minutes with alert/day: {minute_alerts_per_day:.1f}\")\n",
    "    print(f\"  Alert EVENTS/day:       {event_alerts_day:.2f} (events={event_alerts})\")\n",
    "\n",
    "    print(\"\\nIncident EVENT detection:\")\n",
    "    print(f\"  Detected incidents: {det}/{tot} = {inc_event_recall:.3f}\")\n",
    "\n",
    "    return {\n",
    "    \"precision\": float(precision),\n",
    "    \"recall_timestep\": float(recall),\n",
    "    \"f1\": float(f1),\n",
    "\n",
    "    \"tn\": int(tn), \"fp\": int(fp), \"fn\": int(fn), \"tp\": int(tp),\n",
    "\n",
    "    \"minute_alerts_per_day\": float(minute_alerts_per_day),\n",
    "    \"alert_events_per_day\": float(event_alerts_day),\n",
    "    \"n_alert_events\": int(event_alerts),\n",
    "\n",
    "    \"incident_event_recall\": float(inc_event_recall),\n",
    "    \"incident_events_detected\": int(det),\n",
    "    \"incident_events_total\": int(tot),\n",
    "}\n",
    "\n",
    "res_simple = evaluate_baseline_clean(\n",
    "    baseline_simple, df, incident_intervals, test_start, test_end, \"Simple ANY metric\"\n",
    ")\n",
    "\n",
    "res_voting = evaluate_baseline_clean(\n",
    "    baseline_voting, df, incident_intervals, test_start, test_end, \"Voting hybrid\"\n",
    ")"
   ],
   "id": "a6b8490defc1b8c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVALUATING: Simple ANY metric\n",
      "======================================================================\n",
      "Rule: BaselineRule(cpu_utilization>85, memory_usage>85, request_latency>250, error_rate>10, +DOWN:request_rate<(0.4×median_60m), k=1)\n",
      "Precision=0.942 Recall(timestep)=0.891 F1=0.916\n",
      "TN=2710 FP=15 FN=30 TP=245\n",
      "\n",
      "Alert rate:\n",
      "  Minutes with alert/day: 124.8\n",
      "  Alert EVENTS/day:       3.36 (events=7)\n",
      "\n",
      "Incident EVENT detection:\n",
      "  Detected incidents: 4/4 = 1.000\n",
      "\n",
      "======================================================================\n",
      "EVALUATING: Voting hybrid\n",
      "======================================================================\n",
      "Rule: BaselineRuleVoting(cpu_utilization>80, memory_usage>80, request_latency>200, error_rate>8, votes≥2, k=2, critical=[error_rate, request_latency])\n",
      "Precision=0.880 Recall(timestep)=0.804 F1=0.840\n",
      "TN=2695 FP=30 FN=54 TP=221\n",
      "\n",
      "Alert rate:\n",
      "  Minutes with alert/day: 120.5\n",
      "  Alert EVENTS/day:       4.32 (events=9)\n",
      "\n",
      "Incident EVENT detection:\n",
      "  Detected incidents: 4/4 = 1.000\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:52:46.874038500Z",
     "start_time": "2026-02-16T21:52:46.826280700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_simple = np.asarray(baseline_simple.predict(df.iloc[test_start:test_end]), dtype=int)\n",
    "\n",
    "for inc in incident_intervals:\n",
    "    s, e, sc = int(inc[\"start_idx\"]), int(inc[\"end_idx\"]), inc.get(\"scenario\",\"\")\n",
    "    if e <= test_start or s >= test_end:\n",
    "        continue\n",
    "    ls, le = s - test_start, e - test_start\n",
    "    hit = y_pred_simple[ls:le].any()\n",
    "    print(sc, s, e, \"HIT\" if hit else \"MISS\")\n"
   ],
   "id": "1e08c3d983661c4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_crash 17416 17459 HIT\n",
      "traffic_spike 17968 17996 HIT\n",
      "traffic_spike 18651 18839 HIT\n",
      "backend_failure 19671 19687 HIT\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T21:54:57.855837100Z",
     "start_time": "2026-02-16T21:54:57.779679200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_results = {\n",
    "    \"dataset_version\": str(DATASET_VERSION),\n",
    "    \"test_timesteps\": int(len(df_test)),\n",
    "    \"test_incident_timesteps\": int((df_test[\"incident_label\"] == 1).sum()),\n",
    "\n",
    "    \"simple_threshold_plus_drop\": {\n",
    "        \"thresholds_up\": thresholds_simple,\n",
    "        \"k_consecutive\": int(getattr(baseline_simple, \"k_consecutive\", 1)),\n",
    "        \"request_rate_drop\": getattr(baseline_simple, \"request_rate_drop\", None),\n",
    "\n",
    "        \"metrics\": {\n",
    "            \"precision\": res_simple[\"precision\"],\n",
    "            \"recall_timestep\": res_simple[\"recall_timestep\"],\n",
    "            \"f1\": res_simple[\"f1\"],\n",
    "            \"alert_minutes_per_day\": res_simple[\"minute_alerts_per_day\"],\n",
    "            \"alert_events_per_day\": res_simple[\"alert_events_per_day\"],\n",
    "            \"incident_event_recall\": res_simple[\"incident_event_recall\"],\n",
    "            \"incident_events_detected\": res_simple[\"incident_events_detected\"],\n",
    "            \"incident_events_total\": res_simple[\"incident_events_total\"],\n",
    "        },\n",
    "        \"confusion_matrix\": {\n",
    "            \"tp\": res_simple[\"tp\"],\n",
    "            \"fp\": res_simple[\"fp\"],\n",
    "            \"tn\": res_simple[\"tn\"],\n",
    "            \"fn\": res_simple[\"fn\"],\n",
    "        },\n",
    "    },\n",
    "\n",
    "    \"voting_hybrid\": {\n",
    "        \"thresholds\": thresholds_voting,\n",
    "        \"votes_required\": int(getattr(baseline_voting, \"votes_required\", 2)),\n",
    "        \"k_consecutive\": int(getattr(baseline_voting, \"k_consecutive\", getattr(baseline_voting, \"k_consecutive_votes\", 1))),\n",
    "        \"critical_metrics\": sorted(list(getattr(baseline_voting, \"critical_metrics\", []))),\n",
    "\n",
    "        \"metrics\": {\n",
    "            \"precision\": res_voting[\"precision\"],\n",
    "            \"recall_timestep\": res_voting[\"recall_timestep\"],\n",
    "            \"f1\": res_voting[\"f1\"],\n",
    "            \"alert_minutes_per_day\": res_voting[\"minute_alerts_per_day\"],\n",
    "            \"alert_events_per_day\": res_voting[\"alert_events_per_day\"],\n",
    "            \"incident_event_recall\": res_voting[\"incident_event_recall\"],\n",
    "            \"incident_events_detected\": res_voting[\"incident_events_detected\"],\n",
    "            \"incident_events_total\": res_voting[\"incident_events_total\"],\n",
    "        },\n",
    "        \"confusion_matrix\": {\n",
    "            \"tp\": res_voting[\"tp\"],\n",
    "            \"fp\": res_voting[\"fp\"],\n",
    "            \"tn\": res_voting[\"tn\"],\n",
    "            \"fn\": res_voting[\"fn\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "output_file = f\"../results/baseline_results_{DATASET_VERSION}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(baseline_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved: {output_file}\")\n",
    "\n",
    "best = \"simple_threshold_plus_drop\" if baseline_results[\"simple_threshold_plus_drop\"][\"metrics\"][\"f1\"] >= baseline_results[\"voting_hybrid\"][\"metrics\"][\"f1\"] else \"voting_hybrid\"\n",
    "m = baseline_results[best][\"metrics\"]\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(f\"Best baseline: {best}\")\n",
    "print(f\"  Recall(timestep): {m['recall_timestep']:.3f}\")\n",
    "print(f\"  Precision:        {m['precision']:.3f}\")\n",
    "print(f\"  Alert events/day: {m['alert_events_per_day']:.2f}\")\n",
    "print(f\"  Event recall:     {m['incident_event_recall']:.3f}\")"
   ],
   "id": "a4793a5e9c164a2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: baseline_results_v2.json\n",
      "FINAL SUMMARY\n",
      "Best baseline: simple_threshold_plus_drop\n",
      "  Recall(timestep): 0.891\n",
      "  Precision:        0.942\n",
      "  Alert events/day: 3.36\n",
      "  Event recall:     1.000\n"
     ]
    }
   ],
   "execution_count": 70
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
